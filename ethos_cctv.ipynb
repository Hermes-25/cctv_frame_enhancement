{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2680055,"sourceType":"datasetVersion","datasetId":2946}],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T11:46:31.609895Z","iopub.execute_input":"2024-10-01T11:46:31.610677Z","iopub.status.idle":"2024-10-01T11:46:31.615316Z","shell.execute_reply.started":"2024-10-01T11:46:31.610634Z","shell.execute_reply":"2024-10-01T11:46:31.614406Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"markdown","source":"# **Exploring the Dataset**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport glob\nfrom sklearn import cluster","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.617318Z","iopub.execute_input":"2024-10-01T11:46:31.617788Z","iopub.status.idle":"2024-10-01T11:46:31.625602Z","shell.execute_reply.started":"2024-10-01T11:46:31.617739Z","shell.execute_reply":"2024-10-01T11:46:31.624806Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"videoDF = pd.read_csv('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full.csv')\n# videoDF.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.626658Z","iopub.execute_input":"2024-10-01T11:46:31.626936Z","iopub.status.idle":"2024-10-01T11:46:31.641266Z","shell.execute_reply.started":"2024-10-01T11:46:31.626905Z","shell.execute_reply":"2024-10-01T11:46:31.640365Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"# videoDF","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.643165Z","iopub.execute_input":"2024-10-01T11:46:31.643854Z","iopub.status.idle":"2024-10-01T11:46:31.647671Z","shell.execute_reply.started":"2024-10-01T11:46:31.643805Z","shell.execute_reply":"2024-10-01T11:46:31.646818Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"# create a dictionary that maps videoIDs to full file paths\n# single file cane be acessed using \"../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_1/youtube_faces_with_keypoints_full_1/AJ_Cook_0.npz\"\nnpzFilesFullPath = glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_*/youtube_faces_*/*.npz')\nvideoIDs = [x.split('/')[-1].split('.')[0] for x in npzFilesFullPath]\nfullPaths = {}\nfor videoID, fullPath in zip(videoIDs, npzFilesFullPath):\n    fullPaths[videoID] = fullPath\n\n# remove from the large csv file all videos that weren't uploaded yet\n# videoDF = videoDF.loc[videoDF.loc[:,'videoID'].isin(fullPaths.keys()),:].reset_index(drop=True)\nprint('Number of Videos is %d' %(videoDF.shape[0]))\nprint('Number of Unique Individuals is %d' %(len(videoDF['personName'].unique())))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.649515Z","iopub.execute_input":"2024-10-01T11:46:31.649908Z","iopub.status.idle":"2024-10-01T11:46:31.675402Z","shell.execute_reply.started":"2024-10-01T11:46:31.649863Z","shell.execute_reply":"2024-10-01T11:46:31.674507Z"},"trusted":true},"execution_count":271,"outputs":[{"name":"stdout","text":"Number of Videos is 2194\nNumber of Unique Individuals is 828\n","output_type":"stream"}]},{"cell_type":"code","source":"# # overview of the contents of the dataset\n# groupedByPerson = videoDF.groupby(\"personName\")\n# numVidsPerPerson = groupedByPerson.count()['videoID']\n# groupedByPerson.count().sort_values('videoID', axis=0, ascending=False)\n# plt.close('all')\n# plt.figure(figsize=(25,20))\n# plt.subplot(2,2,1)\n# plt.hist(x=numVidsPerPerson,bins=0.5+np.arange(numVidsPerPerson.min()-1,numVidsPerPerson.max()+1))\n# plt.title('Number of Videos per Person',fontsize=30); \n# plt.xlabel('Number of Videos',fontsize=25); plt.ylabel('Number of People',fontsize=25)\n\n# plt.subplot(2,2,2)\n# plt.hist(x=videoDF['videoDuration'],bins=28);\n# plt.title('Distribution of Video Duration',fontsize=30); \n# plt.xlabel('duration [frames]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25)\n# plt.xlim(videoDF['videoDuration'].min()-2,videoDF['videoDuration'].max()+2)\n\n# plt.subplot(2,2,3)\n# plt.scatter(x=videoDF['imageWidth'], y=videoDF['imageHeight'])\n# plt.title('Distribution of Image Sizes',fontsize=30)\n# plt.xlabel('Image Width [pixels]',fontsize=25); plt.ylabel('Image Height [pixels]',fontsize=25)\n# plt.xlim(0,videoDF['imageWidth'].max() +15)\n# plt.ylim(0,videoDF['imageHeight'].max()+15)\n\n# plt.subplot(2,2,4)\n# averageFaceSize_withoutNaNs = np.array(videoDF['averageFaceSize'])\n# averageFaceSize_withoutNaNs = averageFaceSize_withoutNaNs[np.logical_not(np.isnan(averageFaceSize_withoutNaNs))]\n# plt.hist(averageFaceSize_withoutNaNs, bins=28)\n# plt.title('Distribution of Average Face Sizes ',fontsize=30)\n# plt.xlabel('Average Face Size [pixels]',fontsize=25); plt.ylabel('Number of Videos',fontsize=25);","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.676347Z","iopub.execute_input":"2024-10-01T11:46:31.676636Z","iopub.status.idle":"2024-10-01T11:46:31.682537Z","shell.execute_reply.started":"2024-10-01T11:46:31.676599Z","shell.execute_reply":"2024-10-01T11:46:31.681506Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"# # show several frames from each video and overlay 2D keypoints\n# np.random.seed(3)\n# numVideos = 4\n# framesToShowFromVideo = np.array([0.1,0.5,0.9])\n# numFramesPerVideo = len(framesToShowFromVideo)\n\n# # define which points need to be connected with a line\n# jawPoints          = [ 0,17]\n# rigthEyebrowPoints = [17,22]\n# leftEyebrowPoints  = [22,27]\n# noseRidgePoints    = [27,31]\n# noseBasePoints     = [31,36]\n# rightEyePoints     = [36,42]\n# leftEyePoints      = [42,48]\n# outerMouthPoints   = [48,60]\n# innerMouthPoints   = [60,68]\n\n# listOfAllConnectedPoints = [jawPoints,rigthEyebrowPoints,leftEyebrowPoints,\n#                             noseRidgePoints,noseBasePoints,\n#                             rightEyePoints,leftEyePoints,outerMouthPoints,innerMouthPoints]\n\n# # select a random subset of 'numVideos' from the available videos\n# randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']\n\n# fig, axArray = plt.subplots(nrows=numVideos,ncols=numFramesPerVideo,figsize=(14,18))\n# for i, videoID in enumerate(randVideoIDs):\n# # load video\n#     videoFile = np.load(fullPaths[videoID])\n#     colorImages = videoFile['colorImages']\n#     boundingBox = videoFile['boundingBox']\n#     landmarks2D = videoFile['landmarks2D']\n#     landmarks3D = videoFile['landmarks3D']\n\n#     # select frames and show their content\n#     selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)\n#     for j, frameInd in enumerate(selectedFrames):\n#         axArray[i][j].imshow(colorImages[:,:,:,frameInd])\n#         axArray[i][j].scatter(x=landmarks2D[:,0,frameInd],y=landmarks2D[:,1,frameInd],s=9,c='r')\n#         for conPts in listOfAllConnectedPoints:\n#             xPts = landmarks2D[conPts[0]:conPts[-1],0,frameInd]\n#             yPts = landmarks2D[conPts[0]:conPts[-1],1,frameInd]\n#             axArray[i][j].plot(xPts,yPts,c='w',lw=1)\n#         axArray[i][j].set_title('\"%s\" (t=%d)' %(videoID,frameInd), fontsize=12)\n#         axArray[i][j].set_axis_off()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.818985Z","iopub.execute_input":"2024-10-01T11:46:31.819277Z","iopub.status.idle":"2024-10-01T11:46:31.824686Z","shell.execute_reply.started":"2024-10-01T11:46:31.819246Z","shell.execute_reply":"2024-10-01T11:46:31.823766Z"},"trusted":true},"execution_count":273,"outputs":[]},{"cell_type":"markdown","source":"Without the key points -","metadata":{}},{"cell_type":"code","source":"# np.random.seed(3)\n# numVideos = 4\n# framesToShowFromVideo = np.array([0.1,0.5,0.9])\n# numFramesPerVideo = len(framesToShowFromVideo)\n\n# # select a random subset of 'numVideos' from the available videos\n# randVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']\n\n# fig, axArray = plt.subplots(nrows=numVideos,ncols=numFramesPerVideo,figsize=(14,18))\n# for i, videoID in enumerate(randVideoIDs):\n#     # load video\n#     videoFile = np.load(fullPaths[videoID])\n#     colorImages = videoFile['colorImages']\n    \n#     # select frames and show their content\n#     selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)\n#     for j, frameInd in enumerate(selectedFrames):\n#         axArray[i][j].imshow(colorImages[:,:,:,frameInd])\n#         axArray[i][j].set_title('\"%s\" (t=%d)' %(videoID,frameInd), fontsize=12)\n#         axArray[i][j].set_axis_off()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.826358Z","iopub.execute_input":"2024-10-01T11:46:31.826692Z","iopub.status.idle":"2024-10-01T11:46:31.835828Z","shell.execute_reply.started":"2024-10-01T11:46:31.826660Z","shell.execute_reply":"2024-10-01T11:46:31.834933Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# #TPU->\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# # Set device for TPU\n# device = xm.xla_device()\n# print(\"Using device TPU!\")\n\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.836947Z","iopub.execute_input":"2024-10-01T11:46:31.837243Z","iopub.status.idle":"2024-10-01T11:46:31.849248Z","shell.execute_reply.started":"2024-10-01T11:46:31.837211Z","shell.execute_reply":"2024-10-01T11:46:31.848214Z"},"trusted":true},"execution_count":275,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class NumpyVideoDataset(Dataset):\n    def __init__(self, video_df, full_paths, frame_buffer_size=5):\n        print(\"Called Dataset Class and Normalising\")\n        self.video_df = video_df\n        self.full_paths = full_paths\n        self.frame_buffer_size = frame_buffer_size\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __len__(self):\n        return len(self.video_df)\n    \n    def __getitem__(self, idx):\n        video_id = self.video_df.iloc[idx]['videoID']\n        video_file = np.load(self.full_paths[video_id])\n        color_images = video_file['colorImages']\n        \n        # Randomly select a starting frame\n        start_frame = np.random.randint(0, color_images.shape[3] - self.frame_buffer_size)\n        \n        frame_buffer = []\n        for i in range(self.frame_buffer_size):\n            frame = color_images[:, :, :, start_frame + i]\n            frame = self.transform(frame)\n            frame_buffer.append(frame)\n       \n        \n        return torch.stack(frame_buffer), video_id\n\n# Create dataset and dataloader\ndataset = NumpyVideoDataset(videoDF, fullPaths)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n# # Example of how to use the dataloader\n# for frames, video_id in dataloader:\n#     print(f\"Video ID: {video_id[0]}\")\n#     print(f\"Frame buffer shape: {frames.shape}\")\n#     break  # Just to show the first batch","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.850255Z","iopub.execute_input":"2024-10-01T11:46:31.850588Z","iopub.status.idle":"2024-10-01T11:46:31.862027Z","shell.execute_reply.started":"2024-10-01T11:46:31.850531Z","shell.execute_reply":"2024-10-01T11:46:31.861052Z"},"trusted":true},"execution_count":276,"outputs":[{"name":"stdout","text":"Called Dataset Class and Normalising\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define a function to unnormalize and convert tensors back to images\ndef unnormalize_and_convert(tensor):\n    # Undo the normalization: mean and std used in the dataset transformation\n    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n    \n    tensor = tensor * std + mean  # Unnormalize\n    return tensor\n\n# Create a function to display frames\ndef show_frames(frames):\n    # Frames is a tensor of shape [batch_size, frame_buffer_size, 3, 256, 256]\n    frames = frames.squeeze(0)  # Remove batch dimension (since batch_size=1)\n    frames = unnormalize_and_convert(frames)  # Unnormalize the frames\n    \n    # Convert tensor to numpy array and move to CPU if necessary\n    frames = frames.permute(0, 2, 3, 1).cpu().numpy()  # Shape: [frame_buffer_size, 256, 256, 3]\n    \n    # Display the frames\n    num_frames = frames.shape[0]\n    fig, ax = plt.subplots(1, num_frames, figsize=(15, 5))\n    \n    for i in range(num_frames):\n        ax[i].imshow(frames[i])\n        ax[i].axis('off')\n    \n    plt.show()\n\n# Example of how to use the dataloader to load and display images\n# for frames, video_id in dataloader:\n#     print(f\"Video ID: {video_id[0]}\")\n#     print(f\"Frame buffer shape: {frames.shape}\")\n    \n#     # Move frames to the appropriate device (GPU or CPU)\n#     frames = frames.to(device)\n    \n#     # Show the frames\n#     show_frames(frames)\n    \n#     break  # Just to show the first batch","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.864081Z","iopub.execute_input":"2024-10-01T11:46:31.864418Z","iopub.status.idle":"2024-10-01T11:46:31.873610Z","shell.execute_reply.started":"2024-10-01T11:46:31.864386Z","shell.execute_reply":"2024-10-01T11:46:31.872747Z"},"trusted":true},"execution_count":277,"outputs":[]},{"cell_type":"code","source":"class FramePreprocessor(nn.Module):\n    def __init__(self):\n        super(FramePreprocessor, self).__init__()\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_frames, channels, height, width)\n        \n        # Normalization is already done in the dataset class, so we skip it here\n        \n        # Ensure the input is in the correct shape and type\n        x = x.float()\n        \n        # Rearrange dimensions for easier processing\n        #x = x.permute(0, 2, 1, 3, 4)  # (batch_size, channels, num_frames, height, width)\n        \n        return x\n\n# Usage\n# preprocessor = FramePreprocessor()\n\n# # Simulate input tensor of shape (1, 5, 3, 256, 256)\n# input_tensor = torch.randn(1, 5, 3, 256, 256)  # Simulated random input\n\n# # Process the input tensor\n# preprocessed_output = preprocessor(input_tensor)\n\n# # Check the output shape\n# print(\"Preprocessed output shape:\", preprocessed_output.shape)  # Should be (1, 3, 5, 256, 256)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.874651Z","iopub.execute_input":"2024-10-01T11:46:31.874942Z","iopub.status.idle":"2024-10-01T11:46:31.886208Z","shell.execute_reply.started":"2024-10-01T11:46:31.874911Z","shell.execute_reply":"2024-10-01T11:46:31.885340Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"markdown","source":"###  x shape original: (batch_size, num_frames,channels, height, width)\n### but for Conv3D, it needs shape of format (batch_size,channels, num_frames, height, width)","metadata":{}},{"cell_type":"markdown","source":"# MODEL ","metadata":{}},{"cell_type":"markdown","source":"# GSTS and Preprocessing","metadata":{}},{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass GSTS(nn.Module):\n    def __init__(self, num_frames, num_groups=16):\n        super(GSTS, self).__init__()\n        self.num_frames = num_frames\n        self.num_groups = num_groups\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_frames, channels, height, width)\n        b, t, c, h, w = x.size()  # Adjusted the unpacking order\n        #print(f\"Input shape: {x.shape}\")\n        \n        # Calculate group size\n        h_group = h // int(np.sqrt(self.num_groups))\n        w_group = w // int(np.sqrt(self.num_groups))\n        \n        # Reshape to group spatial dimensions\n        x = x.view(b, t, c, int(np.sqrt(self.num_groups)), h_group, int(np.sqrt(self.num_groups)), w_group)\n        #print(f\"After grouping shape: {x.shape}\")\n        \n        # Apply shifts\n        out = torch.zeros_like(x)\n        for i in range(t):\n            if i % 2 == 0:  # Even frames: horizontal shift\n                out[:, i, :, :, :, :-1, :] = x[:, i, :, :, :, 1:, :]\n                out[:, i, :, :, :, -1, :] = x[:, i, :, :, :, 0, :]\n            else:  # Odd frames: vertical shift\n                out[:, i, :, :, :-1, :, :] = x[:, i, :, :, 1:, :, :]\n                out[:, i, :, :, -1, :, :] = x[:, i, :, :, 0, :, :]\n        \n        # Reshape back to original dimensions\n        out = out.view(b, t, c, h, w)\n        #print(f\"Output shape: {out.shape}\")\n        \n        return out\n\n# Usage\ngsts = GSTS(num_frames=5)\n\n# Placeholder FramePreprocessor class for demonstration (replace with actual implementation)\nclass FramePreprocessor(nn.Module):\n    def __init__(self):\n        super(FramePreprocessor, self).__init__()\n    \n    def forward(self, x):\n        # x shape: (batch_size, num_frames, channels, height, width)\n        \n        # Normalization is already done in the dataset class, so we skip it here\n        \n        # Ensure the input is in the correct shape and type\n        x = x.float()\n        \n        # Rearrange dimensions for easier processing\n        #x = x.permute(0, 2, 1, 3, 4)  # (batch_size, channels, num_frames, height, width)\n        \n        return x\n\n# Combine preprocessing and GSTS\nclass PreprocessAndGSTS(nn.Module):\n    def __init__(self, num_frames, num_groups=16):\n        super(PreprocessAndGSTS, self).__init__()\n        self.preprocessor = FramePreprocessor()\n        self.gsts = GSTS(num_frames, num_groups)\n        print(\"Preprocessing and GSTS....\")\n    \n    \n    def forward(self, x):\n        x = self.preprocessor(x)\n        x = self.gsts(x)\n        return x\n\n# Usage\n# preprocess_and_gsts = PreprocessAndGSTS(num_frames=5)\n\n# # Example usage with the dataloader\n# # Assuming dataloader provides input in the required format [batch_size, num_frames, channels, height, width]\n# for frames, video_id in dataloader:\n#     print(f\"Original frames shape: {frames.shape}\")\n#     processed_frames = preprocess_and_gsts(frames)\n#     print(f\"Final processed frames shape: {processed_frames.shape}\")\n#     break  # Just to show the first batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.887573Z","iopub.execute_input":"2024-10-01T11:46:31.887921Z","iopub.status.idle":"2024-10-01T11:46:31.904844Z","shell.execute_reply.started":"2024-10-01T11:46:31.887879Z","shell.execute_reply":"2024-10-01T11:46:31.903935Z"},"trusted":true},"execution_count":279,"outputs":[]},{"cell_type":"markdown","source":"## Output of GSTS and preprocessing","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef show_frames(frames, title):\n    \"\"\"\n    Display a grid of frames from a video.\n    \n    Args:\n    frames (torch.Tensor): Tensor of shape (batch_size, num_frames, channels, height, width)\n                           or (num_frames, channels, height, width)\n    title (str): Title for the plot\n    \"\"\"\n    # Move tensor to CPU and convert to numpy array\n    frames = frames.cpu().numpy()\n    \n    # Squeeze out the batch dimension if it's 1\n    if frames.ndim == 5 and frames.shape[0] == 1:\n        frames = frames.squeeze(0)\n    \n    # Get the number of frames\n    num_frames = frames.shape[0]\n    \n    # Create a grid of subplots\n    fig, axes = plt.subplots(1, num_frames, figsize=(20, 4))\n    fig.suptitle(title)\n    \n    for i in range(num_frames):\n        # Transpose the image to (height, width, channels)\n        img = frames[i].transpose(1, 2, 0)\n        \n        # Handle different channel numbers\n        if img.shape[2] == 1:  # Grayscale\n            img = img.squeeze(2)\n        elif img.shape[2] == 3:  # RGB\n            pass\n        elif img.shape[2] >= 4:  # More than 3 channels, take first 3\n            img = img[:, :, :3]\n        \n        # Normalize the image for display\n        img = (img - img.min()) / (img.max() - img.min())\n        \n        # Display the image\n        if img.ndim == 2:  # Grayscale\n            axes[i].imshow(img, cmap='gray')\n        else:  # RGB\n            axes[i].imshow(img)\n        \n        axes[i].axis('off')\n        axes[i].set_title(f'Frame {i+1}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# # Usage example with the dataloader and preprocessing\n# for frames, video_id in dataloader:\n#     print(f\"Original frames shape: {frames.shape}\")\n#     show_frames(frames, \"Original Frames\")\n    \n#     processed_frames = preprocess_and_gsts(frames)\n#     print(f\"Processed frames shape: {processed_frames.shape}\")\n#     show_frames(processed_frames, \"Processed Frames\")\n    \n#     break  # Just to show the first batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.906344Z","iopub.execute_input":"2024-10-01T11:46:31.906863Z","iopub.status.idle":"2024-10-01T11:46:31.918235Z","shell.execute_reply.started":"2024-10-01T11:46:31.906819Z","shell.execute_reply":"2024-10-01T11:46:31.917424Z"},"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction Module with Output","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nclass FeatureExtractionModule(nn.Module):\n    def __init__(self, in_channels=3, base_channels=64):\n        super(FeatureExtractionModule, self).__init__()\n        self.layers = nn.ModuleList()\n        \n        for i in range(5):\n            out_channels = base_channels * (2 ** i)\n            self.layers.append(nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm3d(out_channels),\n                nn.ReLU(inplace=True)\n            ))\n            in_channels = out_channels\n        \n        self.temporal_shift = TemporalShift()\n\n    def forward(self, x):\n        # Change input shape to (batch_size, channels, num_frames, height, width)\n        x = x.permute(0, 2, 1, 3, 4)  # (batch_size, channels, num_frames, height, width)\n        features = []\n        for layer in self.layers:\n            x = layer(x)\n            x = self.temporal_shift(x)\n            features.append(x)\n        \n        return features\n\nclass TemporalShift(nn.Module):\n    def __init__(self, shift_div=8):\n        super(TemporalShift, self).__init__()\n        self.shift_div = shift_div\n\n    def forward(self, x):\n        B, C, T, H, W = x.size()\n        fold = C // self.shift_div\n\n        out = torch.zeros_like(x)\n        out[:, :fold, 1:] = x[:, :fold, :-1]  # shift left\n        out[:, fold: 2 * fold, :-1] = x[:, fold: 2 * fold, 1:]  # shift right\n        out[:, 2 * fold:] = x[:, 2 * fold:]  # no shift\n\n        return out\n\ndef show_frames(frames, title):\n    \"\"\"Display a grid of frames from a video.\"\"\"\n    # Detach the tensor from the computation graph and move it to CPU\n    frames = frames.detach().cpu().numpy()  \n    \n    if frames.ndim == 5 and frames.shape[0] == 1:\n        frames = frames.squeeze(0)\n    \n    num_frames = frames.shape[0]\n    \n    fig, axes = plt.subplots(1, num_frames, figsize=(20, 4))\n    fig.suptitle(title)\n    \n    for i in range(num_frames):\n        img = frames[i].transpose(1, 2, 0)\n        if img.shape[2] == 1:  \n            img = img.squeeze(2)\n        elif img.shape[2] >= 4:  \n            img = img[:, :, :3]\n        \n        img = (img - img.min()) / (img.max() - img.min())\n        \n        if img.ndim == 2:  \n            axes[i].imshow(img, cmap='gray')\n        else:  \n            axes[i].imshow(img)\n        \n        axes[i].axis('off')\n        axes[i].set_title(f'Frame {i+1}')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# Usage with dataloader\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# fem = FeatureExtractionModule().to(device)\n\n# # Example usage with the dataloader\n# for frames, video_id in dataloader:\n#     print(f\"Original frames shape: {frames.shape}\")\n    \n#     # Display original frames\n#     #show_frames(frames, \"Original Frames\")\n    \n#     # Process frames through the feature extraction module\n#     features = fem(frames.to(device))\n    \n#     print(f\"Number of feature maps: {len(features)}\")\n#     for i, feature in enumerate(features):\n#         print(f\"Feature map {i+1} shape: {feature.shape}\")\n#         # Optionally display feature maps as well\n#         #show_frames(feature, f\"Feature Map {i+1}\")\n\n#     # Uncomment to visualize feature maps\n#     #show_frames(features[-1], \"Last Feature Map\")\n\n#     break  # Just to show the first batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.919313Z","iopub.execute_input":"2024-10-01T11:46:31.919659Z","iopub.status.idle":"2024-10-01T11:46:31.937486Z","shell.execute_reply.started":"2024-10-01T11:46:31.919626Z","shell.execute_reply":"2024-10-01T11:46:31.936738Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"markdown","source":"* The number of feature maps printed corresponds to the number of channels generated by each layer, not the number of frames. The channels encapsulate the features extracted from the entire sequence of frames, providing a more comprehensive representation of the data rather than individual maps for each frame.\n* \n* The conversion from features to images happens in the Facial Reconstruction Module (FRM). This module takes the fused enhanced features and uses a U-Net architecture with attention mechanism to generate the final reconstructed facial images.","metadata":{}},{"cell_type":"markdown","source":"# Attention Fusion of Features with Output","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nclass AttentionFusion(nn.Module):\n    def __init__(self, feature_channels, output_channels):\n        super(AttentionFusion, self).__init__()\n        self.feature_channels = feature_channels  # [64, 128, 256, 512, 1024]\n        self.output_channels = output_channels\n        print(\"Attention Fusing the Features....\")\n\n        # Spatial-temporal attention for each feature map\n        self.attention_layers = nn.ModuleList([\n            nn.Conv3d(channels, 1, kernel_size=1) for channels in feature_channels\n        ])\n\n        # Feature reduction layers\n        self.reduction_layers = nn.ModuleList([\n            nn.Conv3d(channels, output_channels, kernel_size=1) for channels in feature_channels\n        ])\n\n        # Final fusion layer\n        self.fusion_layer = nn.Conv3d(output_channels * len(feature_channels), output_channels, kernel_size=1)\n\n    def forward(self, features):\n        # features: list of 5 tensors with shapes [B, C, T, H, W]\n        attended_features = []\n\n        for i, feature in enumerate(features):\n            # Apply spatial-temporal attention\n            attention = self.attention_layers[i](feature)  # [B, 1, T, H, W]\n            attention_weights = F.softmax(attention.view(attention.size(0), -1), dim=1).view(attention.size())\n            attended_feature = feature * attention_weights\n\n            # Reduce feature channels\n            reduced_feature = self.reduction_layers[i](attended_feature)  # [B, output_channels, T, H, W]\n            attended_features.append(reduced_feature)\n\n        # Concatenate all attended features\n        concat_features = torch.cat(attended_features, dim=1)  # [B, output_channels * 5, T, H, W]\n\n        # Final fusion\n        fused_features = self.fusion_layer(concat_features)  # [B, output_channels, T, H, W]\n\n        return fused_features\n\n# Assuming you have a DataLoader named 'dataloader' that returns frames\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# feature_channels = [64, 128, 256, 512, 1024]\n# output_channels = 64  # You can adjust this based on your needs -> reduced to 64\n# attention_fusion = AttentionFusion(feature_channels, output_channels).to(device)\n\n# # Example usage with the DataLoader\n# for frames, video_id in dataloader:\n#     print(f\"Original frames shape: {frames.shape}\")  # Check the shape of the frames\n\n\n\n#     # Create sample features from frames (replace with your actual feature extraction logic)\n#     sample_features = features\n\n#     fused_features = attention_fusion(sample_features)\n#     print(f\"Fused features shape: {fused_features.shape}\")\n# #     show_frames(fused_features,\"Fused Features\")\n\n#     break  # Remove this if you want to process all batches","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:31.939497Z","iopub.execute_input":"2024-10-01T11:46:31.939848Z","iopub.status.idle":"2024-10-01T11:46:31.950861Z","shell.execute_reply.started":"2024-10-01T11:46:31.939802Z","shell.execute_reply":"2024-10-01T11:46:31.950050Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"markdown","source":"# Enhancement Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels):\n        super(UNet, self).__init__()\n        self.encoder1 = ConvBlock(in_channels, 64)\n        self.encoder2 = ConvBlock(64, 128)\n        self.encoder3 = ConvBlock(128, 256)\n        self.bottleneck = ConvBlock(256, 512)\n\n        self.decoder3 = ConvBlock(512, 256)\n        self.decoder2 = ConvBlock(256, 128)\n        self.decoder1 = ConvBlock(128, 64)\n\n        self.final_conv = nn.Conv3d(64, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(F.max_pool3d(enc1, kernel_size=(1, 2, 2)))  # Pool only spatially\n        enc3 = self.encoder3(F.max_pool3d(enc2, kernel_size=(1, 2, 2)))\n\n        bottleneck = self.bottleneck(F.max_pool3d(enc3, kernel_size=(1, 2, 2)))\n\n        dec3 = self.decoder3(F.interpolate(bottleneck, scale_factor=(1, 2, 2), mode='trilinear', align_corners=False))\n        dec3 = F.interpolate(dec3, size=enc3.shape[2:], mode='trilinear', align_corners=False)  # Ensure spatial match\n        dec3 = dec3 + enc3  # Skip connection\n        \n        dec2 = self.decoder2(F.interpolate(dec3, scale_factor=(1, 2, 2), mode='trilinear', align_corners=False))\n        dec2 = F.interpolate(dec2, size=enc2.shape[2:], mode='trilinear', align_corners=False)  # Ensure spatial match\n        dec2 = dec2 + enc2  # Skip connection\n        \n        dec1 = self.decoder1(F.interpolate(dec2, scale_factor=(1, 2, 2), mode='trilinear', align_corners=False))\n        dec1 = F.interpolate(dec1, size=enc1.shape[2:], mode='trilinear', align_corners=False)  # Ensure spatial match\n        dec1 = dec1 + enc1  # Skip connection\n\n        return self.final_conv(dec1)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, channels):\n        super(AttentionBlock, self).__init__()\n        self.conv = nn.Conv3d(channels, 1, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        attention = self.sigmoid(self.conv(x))\n        return x * attention\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass DeblurringModule(nn.Module):\n    def __init__(self, in_channels):\n        super(DeblurringModule, self).__init__()\n        self.skip_connection = nn.Sequential(\n            ConvBlock(in_channels, in_channels // 2),\n            nn.Conv3d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1)\n        )\n        self.decoder_blocks = nn.ModuleList([\n            ConvBlock(in_channels, in_channels // 2),  # First block after skip connection\n            ConvBlock(in_channels // 2, in_channels // 4),\n            ConvBlock(in_channels // 4, in_channels // 8),\n            ConvBlock(in_channels // 8, in_channels // 8)\n        ])\n\n    def forward(self, x):\n        # Create a skip connection\n        skip_connection = self.skip_connection(x)\n        \n        # If dimensions don't match, adjust\n        if x.shape[1] != skip_connection.shape[1]:\n            skip_connection = nn.Conv3d(skip_connection.shape[1], x.shape[1], kernel_size=1)(skip_connection)\n        \n        # Concatenate along the channel dimension\n        x = torch.cat([x, skip_connection], dim=1)\n        \n        # Reduce channels back to original after concatenation\n        x = nn.Conv3d(x.shape[1], x.shape[1] // 2, kernel_size=1)(x)\n        \n        # Pass through the decoder blocks\n        for block in self.decoder_blocks:\n            x = block(x)\n        \n        return x\n\n# Example usage\n# deblur_module = DeblurringModule(in_channels=256)\n# output = deblur_module(torch.randn(1, 256, 5, 256, 256))  # Sample input\n\n\n\nclass SuperResolutionModule(nn.Module):\n    def __init__(self, in_channels, upscale_factor=4):\n        super(SuperResolutionModule, self).__init__()\n        self.upscale_factor = upscale_factor\n        self.conv1 = nn.Conv3d(in_channels, in_channels * upscale_factor ** 2, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv3d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.attention = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        batch, channels, time, height, width = x.shape\n        \n        # Apply 3D convolution\n        x = F.relu(self.conv1(x))\n        \n        # Reshape and permute for pixel shuffle\n        x = x.view(batch, -1, self.upscale_factor, self.upscale_factor, time, height, width)\n        x = x.permute(0, 1, 5, 2, 6, 3, 4).contiguous()\n        x = x.view(batch, channels, time, height * self.upscale_factor, width * self.upscale_factor)\n        \n        x = F.relu(self.conv2(x))\n        x = self.attention(x)\n        return x\n\n\n# class AdvancedEnhancementModule(nn.Module):\n#     def __init__(self, in_channels=64):\n#         super(AdvancedEnhancementModule, self).__init__()\n#         self.initial_conv = ConvBlock(in_channels, in_channels)\n#         self.low_light = UNet(in_channels)\n#         self.deblur = DeblurringModule(in_channels)\n#         self.super_res = SuperResolutionModule(in_channels, upscale_factor=4)\n#         print(\"Enhacing the features....\")\n\n#     def forward(self, x):\n#         x = self.initial_conv(x)\n        \n#         low_light = self.low_light(x)\n#         deblur = self.deblur(x)\n#         super_res = self.super_res(x)\n\n#         # Ensure all outputs have the same spatial dimensions as super_res\n#         low_light = F.interpolate(low_light, size=super_res.shape[2:], mode='trilinear', align_corners=False)\n#         deblur = F.interpolate(deblur, size=super_res.shape[2:], mode='trilinear', align_corners=False)\n\n#         # Ensure all outputs have the same number of channels\n#         if low_light.shape[1] != in_channels:\n#             low_light = nn.Conv3d(low_light.shape[1], in_channels, kernel_size=1)(low_light)\n#         if deblur.shape[1] != in_channels:\n#             deblur = nn.Conv3d(deblur.shape[1], in_channels, kernel_size=1)(deblur)\n#         if super_res.shape[1] != in_channels:\n#             super_res = nn.Conv3d(super_res.shape[1], in_channels, kernel_size=1)(super_res)\n\n#         return low_light, deblur, super_res\n\nclass AdvancedEnhancementModule(nn.Module):\n    def __init__(self, in_channels=64):\n        super(AdvancedEnhancementModule, self).__init__()\n        self.initial_conv = ConvBlock(in_channels, in_channels)\n        self.low_light = UNet(in_channels)\n        self.deblur = DeblurringModule(in_channels)\n        self.super_res = SuperResolutionModule(in_channels, upscale_factor=4)\n        print(\"Enhancing the features...\")\n\n    def to(self, device):\n        super(AdvancedEnhancementModule, self).to(device)\n        self.initial_conv.to(device)\n        self.low_light.to(device)\n        self.deblur.to(device)\n        self.super_res.to(device)\n        return self\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        \n        low_light = self.low_light(x)\n        deblur = self.deblur(x)\n        super_res = self.super_res(x)\n\n        # Ensure all outputs have the same spatial dimensions as super_res\n        low_light = F.interpolate(low_light, size=super_res.shape[2:], mode='trilinear', align_corners=False)\n        deblur = F.interpolate(deblur, size=super_res.shape[2:], mode='trilinear', align_corners=False)\n\n        # Ensure all outputs have the same number of channels\n        if low_light.shape[1] != x.shape[1]:\n            low_light = nn.Conv3d(low_light.shape[1], x.shape[1], kernel_size=1).to(x.device)(low_light)\n        if deblur.shape[1] != x.shape[1]:\n            deblur = nn.Conv3d(deblur.shape[1], x.shape[1], kernel_size=1).to(x.device)(deblur)\n        if super_res.shape[1] != x.shape[1]:\n            super_res = nn.Conv3d(super_res.shape[1], x.shape[1], kernel_size=1).to(x.device)(super_res)\n\n        return low_light, deblur, super_res\n\n# # Usage example\n# if __name__ == \"__main__\":\n#     in_channels = 64\n#     enhancement_module = AdvancedEnhancementModule(in_channels)\n    \n#     # Sample input with 5 time slices and (256x256) image size\n#     input_tensor = torch.randn(1, 64, 5, 256, 256)\n    \n#     low_light, deblur, super_res = enhancement_module(input_tensor)\n    \n#     print(f\"Input shape: {input_tensor.shape}\")\n#     print(f\"Low Light Enhancement shape: {low_light.shape}\")\n#     print(f\"Deblurring shape: {deblur.shape}\")\n#     print(f\"Super Resolution shape: {super_res.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.027797Z","iopub.execute_input":"2024-10-01T11:46:32.028093Z","iopub.status.idle":"2024-10-01T11:46:32.067169Z","shell.execute_reply.started":"2024-10-01T11:46:32.028062Z","shell.execute_reply":"2024-10-01T11:46:32.066343Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"markdown","source":"# Fusion of Outputs of Subnets","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EnhancedFusionModule(nn.Module):\n    def __init__(self, in_channels=64, mid_channels=64, out_channels=64):\n        super(EnhancedFusionModule, self).__init__()\n        \n        self.conv_deblur = nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1)\n        self.conv_lowlight = nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1)\n        self.conv_superres = nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1)\n        \n        self.attention = nn.Sequential(\n            nn.Conv3d(mid_channels * 3, mid_channels * 3, kernel_size=1),\n            nn.Sigmoid()\n        )\n        print(\"Fusing the Outputs of the EM....\")\n        \n        self.final_conv = nn.Conv3d(mid_channels * 3, out_channels, kernel_size=1)\n        \n        self.bn = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, deblurred, lowlight, superres):\n        # Inputs are already in shape (batch, channel, time, height, width)\n        \n        # Resize inputs to match the largest size (super-resolution)\n        deblurred = F.interpolate(deblurred, size=superres.shape[2:], mode='trilinear', align_corners=False)\n        lowlight = F.interpolate(lowlight, size=superres.shape[2:], mode='trilinear', align_corners=False)\n        \n        # Apply 3D convolutions\n        deblurred_feat = self.conv_deblur(deblurred)\n        lowlight_feat = self.conv_lowlight(lowlight)\n        superres_feat = self.conv_superres(superres)\n        \n        # Concatenate features\n        combined_feat = torch.cat([deblurred_feat, lowlight_feat, superres_feat], dim=1)\n        \n        # Compute and apply attention weights\n        attention_weights = self.attention(combined_feat)\n        attended_feat = combined_feat * attention_weights\n        \n        # Final fusion\n        out = self.final_conv(attended_feat)\n        out = self.bn(out)\n        out = self.relu(out)\n        \n        return out\n\n# # Example usage\n# fusion_module = EnhancedFusionModule(in_channels=64, mid_channels=64, out_channels=64)\n\n# # Example input tensors (batch, channel, time, height, width)\n# deblurred = torch.randn(1, 64, 5, 256, 256)\n# lowlight = torch.randn(1, 64, 5, 256, 256)\n# superres = torch.randn(1, 64, 5, 1024, 1024)  # Assuming 4x upscaling\n\n# # Forward pass\n# fused_features = fusion_module(deblurred, lowlight, superres)\n# print(f\"Fused features shape: {fused_features.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.068877Z","iopub.execute_input":"2024-10-01T11:46:32.069159Z","iopub.status.idle":"2024-10-01T11:46:32.080847Z","shell.execute_reply.started":"2024-10-01T11:46:32.069128Z","shell.execute_reply":"2024-10-01T11:46:32.079885Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"markdown","source":"# FRM- Reconstructing the Image/Frames","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EfficientAttentionBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(EfficientAttentionBlock, self).__init__()\n        self.conv = nn.Conv3d(in_channels, 1, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        attention = self.sigmoid(self.conv(x))\n        return x * attention\n\nclass EfficientUNet3D(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(EfficientUNet3D, self).__init__()\n        \n        # Reduce number of channels\n        self.enc1 = self.conv_block_3d(in_channels, 64)\n        self.pool1 = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n        \n        self.enc2 = self.conv_block_3d(64, 128)\n        self.pool2 = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n        \n        self.enc3 = self.conv_block_3d(128, 256)\n        \n        # Decoder\n        self.upconv2 = nn.ConvTranspose3d(256, 128, kernel_size=(1,2,2), stride=(1,2,2))\n        self.dec2 = self.conv_block_3d(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose3d(128, 64, kernel_size=(1,2,2), stride=(1,2,2))\n        self.dec1 = self.conv_block_3d(128, 64)\n        \n        self.final_conv = nn.Conv3d(64, out_channels, kernel_size=1)\n        \n        # Efficient Attention blocks\n        self.attention1 = EfficientAttentionBlock(64)\n        self.attention2 = EfficientAttentionBlock(128)\n        self.attention3 = EfficientAttentionBlock(256)\n\n    def conv_block_3d(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.attention1(self.enc1(x))\n        \n        enc2 = self.attention2(self.enc2(self.pool1(enc1)))\n        \n        enc3 = self.attention3(self.enc3(self.pool2(enc2)))\n        \n        # Decoder\n        dec2 = self.upconv2(enc3)\n        dec2 = self.dec2(torch.cat((dec2, enc2), dim=1))\n        \n        dec1 = self.upconv1(dec2)\n        dec1 = self.dec1(torch.cat((dec1, enc1), dim=1))\n        \n        return self.final_conv(dec1)\n\nclass OptimizedFacialReconstructionModule(nn.Module):\n    def __init__(self, in_channels=64, out_channels=3):\n        super(OptimizedFacialReconstructionModule, self).__init__()\n        self.unet3d = EfficientUNet3D(in_channels, out_channels)\n        print(\"FRM....\")\n\n    def forward(self, fused_enhanced_features):\n        # Reduce spatial dimensions\n        x = F.interpolate(fused_enhanced_features, scale_factor=(1, 0.5, 0.5), mode='trilinear', align_corners=False)\n        reconstructed_frames = self.unet3d(x)\n        # Upscale back to original size\n        return F.interpolate(reconstructed_frames, size=fused_enhanced_features.shape[2:], mode='trilinear', align_corners=False)\n\n# # Example usage\n# if __name__ == \"__main__\":\n#     frm = OptimizedFacialReconstructionModule(in_channels=64, out_channels=3)\n#     fused_features = torch.randn(1, 64, 5, 1024, 1024)\n#     reconstructed_frames = frm(fused_features)\n#     print(f\"Reconstructed frames shape: {reconstructed_frames.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.082174Z","iopub.execute_input":"2024-10-01T11:46:32.082454Z","iopub.status.idle":"2024-10-01T11:46:32.101114Z","shell.execute_reply.started":"2024-10-01T11:46:32.082416Z","shell.execute_reply":"2024-10-01T11:46:32.100138Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"markdown","source":"### Output Middle Frame -> No need of this as we need to show all the enhanced 5 frames","metadata":{}},{"cell_type":"code","source":"# def output_middle_frame(reconstructed_frames):\n#     # Check the dimensionality of the input\n#     if reconstructed_frames.dim() == 5:\n#         # 5D input: [batch_size, channels, frames, height, width]\n#         middle_frame_index = reconstructed_frames.shape[2] // 2  # Frames dimension is at index 2\n#         middle_frame = reconstructed_frames[:, :, middle_frame_index, :, :]\n#     elif reconstructed_frames.dim() == 4:\n#         # 4D input: [batch_size, channels, height, width]\n#         # Already a single frame, no need to select middle\n#         middle_frame = reconstructed_frames\n#     else:\n#         raise ValueError(f\"Unexpected input shape: {reconstructed_frames.shape}\")\n    \n#     return middle_frame\n\n# # Example usage\n# import torch\n\n# # Create a sample 5D tensor with shape [1, 3, 5, 256, 256]\n# reconstructed_frames = torch.randn(1, 3, 5, 1024, 1024)\n\n# # Get the middle frame\n# middle_frame = output_middle_frame(reconstructed_frames)\n\n# print(f\"Middle frame shape: {middle_frame.shape}\")\n\n# # This should print: Middle frame shape: torch.Size([1, 3, 256, 256])\n\n\n# WE NEED TO SHOW ALL 5 FRAMES of the VIDEO!","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.102323Z","iopub.execute_input":"2024-10-01T11:46:32.102635Z","iopub.status.idle":"2024-10-01T11:46:32.111029Z","shell.execute_reply.started":"2024-10-01T11:46:32.102588Z","shell.execute_reply":"2024-10-01T11:46:32.110215Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"markdown","source":"### Creating a buffer -> No need, maybe useful during Realtime Implementation","metadata":{}},{"cell_type":"code","source":"# import torch\n\n# class FrameBuffer:\n#     def __init__(self, buffer_size=5):\n#         self.buffer = []\n#         self.buffer_size = buffer_size\n\n#     def update(self, new_frame):\n#         # Ensure the new_frame is 4D: [1, 1, 256, 256]\n#         if new_frame.dim() == 3:\n#             new_frame = new_frame.unsqueeze(0)\n#         if new_frame.dim() != 4 or new_frame.shape != (1, 3, 256, 256):\n#             raise ValueError(\"New frame must have shape [1, 3, 256, 256]\")\n\n#         if len(self.buffer) >= self.buffer_size:\n#             self.buffer.pop(0)  # Remove the oldest frame\n#         self.buffer.append(new_frame)\n\n#     def get_buffer(self):\n#         if not self.buffer:\n#             return None\n#         # Stack along the batch dimension (dim=0)\n#         return torch.cat(self.buffer, dim=0)\n\n# # Example usage\n# frame_buffer = FrameBuffer()\n\n# # Simulating incoming frames from 2D UNet output\n# for _ in range(10):\n#     new_frame = torch.randn(1, 3, 256, 256)  # Simulated new frame with correct shape\n#     frame_buffer.update(new_frame)\n\n# current_buffer = frame_buffer.get_buffer()\n# print(f\"Current buffer shape: {current_buffer.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.113378Z","iopub.execute_input":"2024-10-01T11:46:32.114114Z","iopub.status.idle":"2024-10-01T11:46:32.121241Z","shell.execute_reply.started":"2024-10-01T11:46:32.114059Z","shell.execute_reply":"2024-10-01T11:46:32.120363Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"markdown","source":"# Running in a Loop","metadata":{}},{"cell_type":"code","source":"# import torch_xla\n# print(torch_xla.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.122437Z","iopub.execute_input":"2024-10-01T11:46:32.122798Z","iopub.status.idle":"2024-10-01T11:46:32.130875Z","shell.execute_reply.started":"2024-10-01T11:46:32.122763Z","shell.execute_reply":"2024-10-01T11:46:32.130052Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.132232Z","iopub.execute_input":"2024-10-01T11:46:32.132782Z","iopub.status.idle":"2024-10-01T11:46:32.139096Z","shell.execute_reply.started":"2024-10-01T11:46:32.132735Z","shell.execute_reply":"2024-10-01T11:46:32.138289Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device\", device)\n\n# Initialize the modules\nin_channels = 64\npreprocess_and_gsts = PreprocessAndGSTS(num_frames=5).to(device)\nfem = FeatureExtractionModule().to(device)\nfeature_channels = [64, 128, 256, 512, 1024]\noutput_channels = 64  # You can adjust this based on your needs -> reduced to 64\nattention_fusion = AttentionFusion(feature_channels, output_channels).to(device)\nenhancement_module = AdvancedEnhancementModule(in_channels=in_channels).to(device)\nfusion_module = EnhancedFusionModule(in_channels=in_channels, mid_channels=64, out_channels=64).to(device)\nfacial_reconstruction_module = OptimizedFacialReconstructionModule(in_channels=64, out_channels=3).to(device)\n\nimport torch\n\ndef print_memory_status(step_name):\n    allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n    reserved = torch.cuda.memory_reserved() / (1024 ** 3)  # Convert to GB\n    print(f\"{step_name} - Allocated: {allocated:.4f} GB, Reserved: {reserved:.4f} GB\")\n\n# Assuming 'dataloader' is defined and provides the actual dataset\nfor batch_idx, (frames, video_id) in enumerate(dataloader):\n    # Move the frames to the device (assuming frames are of shape [B, C, T, H, W])\n    frames = frames.to(device)\n    print_memory_status(\"After loading frames\")\n\n    #1. Pre and GSTS\n    processed_frames = preprocess_and_gsts(frames)\n    print_memory_status(\"After Preprocess and GSTS\")\n    \n    #2. Extract Features\n    features = fem(processed_frames)\n    print_memory_status(\"After Feature Extraction\")\n\n    #3. Attention Fusion of feature maps\n    fused_features = attention_fusion(features)\n    print_memory_status(\"After Attention Fusion\")\n\n    #4. Apply the enhancement module to get the low-light, deblurred, and super-resolved features\n    low_light, deblurred, super_res = enhancement_module(fused_features)\n    print_memory_status(\"After Enhancement Module\")\n\n    #5. Fuse the enhanced features using the fusion module\n    fused_features = fusion_module(deblurred, low_light, super_res)\n    print_memory_status(\"After Fusion Module\")\n\n    #6. Reconstruct the final frames using the facial reconstruction module\n    reconstructed_frames = facial_reconstruction_module(fused_features)\n    print_memory_status(\"After Facial Reconstruction\")\n\n    # Print out the shapes to ensure everything is correct\n    print(f\"Batch {batch_idx}:\")\n    print(f\"Input frames shape: {frames.shape}\")\n    print(f\"Pre and GSTS shape: {processed_frames.shape}\")\n    print(f\"Low-light enhanced features shape: {low_light.shape}\")\n    print(f\"Deblurred features shape: {deblurred.shape}\")\n    print(f\"Super-resolved features shape: {super_res.shape}\")\n    print(f\"Fused features shape: {fused_features.shape}\")\n    print(f\"Reconstructed frames shape: {reconstructed_frames.shape}\")\n    \n    # Here, you could save or visualize the output if necessary.\n    show_frames(reconstructed_frames, f\"Reconstructed Frames for {video_id}\")\n    \n    # Break after one batch if only testing the loop; remove this to run through the entire dataset\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:46:32.140661Z","iopub.execute_input":"2024-10-01T11:46:32.141207Z","iopub.status.idle":"2024-10-01T11:46:32.571986Z","shell.execute_reply.started":"2024-10-01T11:46:32.141156Z","shell.execute_reply":"2024-10-01T11:46:32.570637Z"},"trusted":true},"execution_count":290,"outputs":[{"name":"stdout","text":"Using device cuda\nPreprocessing and GSTS....\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[290], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      7\u001b[0m preprocess_and_gsts \u001b[38;5;241m=\u001b[39m PreprocessAndGSTS(num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m fem \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureExtractionModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m feature_channels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m]\n\u001b[1;32m     10\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# You can adjust this based on your needs -> reduced to 64\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 3889 has 14.71 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 39.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 3889 has 14.71 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 39.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
